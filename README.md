# QBQTC
QBQTC: QQ Browser Query Title Corpus


# æ•°æ®é›†ä»‹ç»
TODO è¿™é‡Œæ˜¯ç®€è¦çš„æ•°æ®é›†ä»‹ç»

# baselineæ•ˆæœå¯¹æ¯”
TODO baselineæ•ˆæœå¯¹æ¯”
| æ¨¡å‹ | è®­ç»ƒé›†ï¼ˆtrain) | éªŒè¯é›†ï¼ˆdev) | æµ‹è¯•é›†ï¼ˆtest) | è®­ç»ƒå‚æ•° |
| :----:| :----: | :----: | :----: | :----: |
| <a href="https://huggingface.co/bert-base-chinese/tree/main">BERT-base</a> | F1:93.2  Acc:93.7 | F1: 64.1 Acc:66.6 | F1: 65.1 Acc:66.2 | batch=64, length=52, epoch=7, lr=2e-5, warmup=0.9 |
|<a href="https://huggingface.co/hfl/chinese-roberta-wwm-ext"> RoBERTa-wwm-ext</a> | F1:89.0 Acc:89.8 | F1:62.3 Acc:64.8 | F1:65.1 Acc:66.2 | batch=64, length=52, epoch=7, lr=2e-5, warmup=0.9|
| <a href="https://huggingface.co/hfl/chinese-roberta-wwm-ext-large">RoBERTa-wwm-large-ext</a> | F1:92.6 Acc:93.0 | F1:61.7 Acc:63.7 | F1:65.1 Acc:65.9 | batch=64, length=52, epoch=7, lr=2e-5, warmup=0.9|

f1_scoreæ¥è‡ªäºsklearn.metricsï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š
`F1 =  2 * (precision * recall) / (precision + recall)`

# ä¸€é”®è¿è¡Œbaseline
ğŸè¿è¡Œç¯å¢ƒï¼špytorch 1.7.1/cuda 11.0 + transformers 3.5.0
---------------------------------------------------------------------
    ä½¿ç”¨æ–¹å¼ï¼š
    1ã€å…‹éš†é¡¹ç›® 
       git clone https://github.com/CLUEbenchmark/QBQTC.git
    2ã€è¿›å…¥åˆ°ç›¸åº”çš„ç›®å½•
      ä¾‹å¦‚ï¼šcd QBQTC/baselines
    3ã€ä¸‹è½½å¯¹åº”ä»»åŠ¡æ¨¡å‹å‚æ•°
    	QBQTC/weights/bert-base-chinese
    	QBQTC/weights/chinese-roberta-wwm-ext
    	QBQTC/weights/chinese-roberta-wwm-ext-large
    4ã€è¿è¡Œå¯¹åº”ä»»åŠ¡çš„æ¨¡å‹(GPUæ–¹å¼): 
       python BERT.py --model_name_or_path ../weights/chinese-roberta-wwm-ext --max_seq_length 52 --batch_size 64 --num_epochs 7 --learning_rate 2e-5 --num_labels 3
       ç®€åŒ–ç‰ˆï¼špython BERT.py

# æ•°æ®é›†ä¾‹å­
TODO æ•°æ®é›†ä¾‹å­

# æäº¤æ ·ä¾‹
TODO æäº¤æ ·ä¾‹
